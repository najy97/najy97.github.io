# environment

1. state 정의 : 1순위
    - 환경 구성요소
    - agent 정보
        - x
        - y
        - 현재 capacity
        - 현재 시각 + 이동 시간 < 이걸로 time window 초과했는지 판단가능하지 않을까

2. reward 정의 : 0순위
    - 거리 = 한 step마다 거리를 잼 ⇒ 거리값이 반드시 필요하다
        - + $\frac{1}{거리}$
    - step
        - 노드 하나 움직이는거 → 이래야 state가 나온다
    - action
        - 노드 하나 움직이는거 → 이동 시간

3. 평가지표
    1. 같은 개수의 음식 배달 시 연료 소모량
    2. 거리가 기준 ⇒ 처리됐을 경우 +1/거리
        - 1명 기준

4. env function -> reset(), step(action) <- state, reward 둘다 필요
self -> 내장 함수 사용하나? 안하더라고 결국에는 위에 두개만 구현하면 바로 적용가능,,
step + self.variable 정의
    1. self 변수
        - self.speed = 속도
        - request = 고객 정보 = 전체 node
            - x
            - y
            - 요구 capacity
            - node type
            - ~~time window = 제한시간 (10시까지 배달해야함 이런거) '시각' 리퀘스트별로~~
        - constraint
            - 제한시간(12시까지 전부 처리)
            - capacity = 제한용량
        - list의 70번에 접근하면 70번 노드의 정보를 가져올 수 있게
            - x,y→graph
            - capacity,time → customer

    2. reset()
        - depot = 시작 위치
        - capacity = 0
        - time = 0

    3. action
        - 노드 to 노드 = 노드 번호로 이동

    4. step(action)
        - request 처리한거는 node_check로 False 처리

        - state 불러오기

        - next_x, next_y를 action값으로 불러오기

        - 더할 time값

        - state update
            - x,y,capacity,time
                - capacity ⇒ 노드 타입별로 다르겠네,,, 가게++, 고객--

        - done
            - capacity 초과하면 cut
            - 시간 초과하면 cut

        - reward
            - + $\frac{1}{거리}$
            - 시간 초과하면 ———————————-
            - capacity 초과하면 ——————————

5. DQN 구조 변경 -> input size <- state 정의하고 <- 제일 마지막
    - state의 개수의 배수 = 그대로 해도 될듯???